2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.a.s.SparkContext - Running Spark version 2.2.0
2019-01-17  WARN [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.a.h.u.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.a.s.SparkContext - Submitted application: spark session
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.a.s.SecurityManager - Changing view acls to: elainetuang
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.a.s.SecurityManager - Changing modify acls to: elainetuang
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.a.s.SecurityManager - Changing view acls groups to: 
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.a.s.SecurityManager - Changing modify acls groups to: 
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.a.s.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(elainetuang); groups with view permissions: Set(); users  with modify permissions: Set(elainetuang); groups with modify permissions: Set()
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.a.s.u.Utils - Successfully started service 'sparkDriver' on port 59380.
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.a.s.SparkEnv - Registering MapOutputTracker
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.a.s.SparkEnv - Registering BlockManagerMaster
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.a.s.s.DiskBlockManager - Created local directory at /private/var/folders/dn/2sn7_b997mqg428h3xf_983w0000gn/T/blockmgr-655d798d-00cd-422a-a6a8-2677e09d620a
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.a.s.s.m.MemoryStore - MemoryStore started with capacity 912.3 MB
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.a.s.SparkEnv - Registering OutputCommitCoordinator
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.s.j.u.log - Logging initialized @3845ms
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.s.j.s.Server - jetty-9.3.z-SNAPSHOT
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.s.j.s.Server - Started @4001ms
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.s.j.s.AbstractConnector - Started ServerConnector@1bd4d050{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.a.s.u.Utils - Successfully started service 'SparkUI' on port 4040.
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.s.j.s.h.ContextHandler - Started o.s.j.s.ServletContextHandler@42c79cb1{/jobs,null,AVAILABLE,@Spark}
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.s.j.s.h.ContextHandler - Started o.s.j.s.ServletContextHandler@3c6d488e{/jobs/json,null,AVAILABLE,@Spark}
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.s.j.s.h.ContextHandler - Started o.s.j.s.ServletContextHandler@383194fb{/jobs/job,null,AVAILABLE,@Spark}
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.s.j.s.h.ContextHandler - Started o.s.j.s.ServletContextHandler@a50bec5{/jobs/job/json,null,AVAILABLE,@Spark}
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.s.j.s.h.ContextHandler - Started o.s.j.s.ServletContextHandler@33cb2f7{/stages,null,AVAILABLE,@Spark}
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.s.j.s.h.ContextHandler - Started o.s.j.s.ServletContextHandler@1fef8eb5{/stages/json,null,AVAILABLE,@Spark}
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.s.j.s.h.ContextHandler - Started o.s.j.s.ServletContextHandler@54d4447b{/stages/stage,null,AVAILABLE,@Spark}
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.s.j.s.h.ContextHandler - Started o.s.j.s.ServletContextHandler@1952292c{/stages/stage/json,null,AVAILABLE,@Spark}
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.s.j.s.h.ContextHandler - Started o.s.j.s.ServletContextHandler@3710a168{/stages/pool,null,AVAILABLE,@Spark}
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.s.j.s.h.ContextHandler - Started o.s.j.s.ServletContextHandler@4a5c0040{/stages/pool/json,null,AVAILABLE,@Spark}
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.s.j.s.h.ContextHandler - Started o.s.j.s.ServletContextHandler@576d73ff{/storage,null,AVAILABLE,@Spark}
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.s.j.s.h.ContextHandler - Started o.s.j.s.ServletContextHandler@45bd0ced{/storage/json,null,AVAILABLE,@Spark}
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.s.j.s.h.ContextHandler - Started o.s.j.s.ServletContextHandler@38586217{/storage/rdd,null,AVAILABLE,@Spark}
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.s.j.s.h.ContextHandler - Started o.s.j.s.ServletContextHandler@30b99487{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.s.j.s.h.ContextHandler - Started o.s.j.s.ServletContextHandler@7f774aa7{/environment,null,AVAILABLE,@Spark}
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.s.j.s.h.ContextHandler - Started o.s.j.s.ServletContextHandler@6ff9e4b7{/environment/json,null,AVAILABLE,@Spark}
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.s.j.s.h.ContextHandler - Started o.s.j.s.ServletContextHandler@8bffb1c{/executors,null,AVAILABLE,@Spark}
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.s.j.s.h.ContextHandler - Started o.s.j.s.ServletContextHandler@79356989{/executors/json,null,AVAILABLE,@Spark}
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.s.j.s.h.ContextHandler - Started o.s.j.s.ServletContextHandler@6667d56{/executors/threadDump,null,AVAILABLE,@Spark}
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.s.j.s.h.ContextHandler - Started o.s.j.s.ServletContextHandler@42804656{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.s.j.s.h.ContextHandler - Started o.s.j.s.ServletContextHandler@367565ec{/static,null,AVAILABLE,@Spark}
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.s.j.s.h.ContextHandler - Started o.s.j.s.ServletContextHandler@787bbf34{/,null,AVAILABLE,@Spark}
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.s.j.s.h.ContextHandler - Started o.s.j.s.ServletContextHandler@18e91ff1{/api,null,AVAILABLE,@Spark}
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.s.j.s.h.ContextHandler - Started o.s.j.s.ServletContextHandler@1baae249{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.s.j.s.h.ContextHandler - Started o.s.j.s.ServletContextHandler@7f516d62{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.a.s.u.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://10.3.0.99:4040
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.a.s.e.Executor - Starting executor ID driver on host localhost
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.a.s.u.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 59381.
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.a.s.n.n.NettyBlockTransferService - Server created on 10.3.0.99:59381
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.a.s.s.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.a.s.s.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 10.3.0.99, 59381, None)
2019-01-17  INFO [dispatcher-event-loop-2] o.a.s.s.BlockManagerMasterEndpoint - Registering block manager 10.3.0.99:59381 with 912.3 MB RAM, BlockManagerId(driver, 10.3.0.99, 59381, None)
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.a.s.s.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 10.3.0.99, 59381, None)
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.a.s.s.BlockManager - Initialized BlockManager: BlockManagerId(driver, 10.3.0.99, 59381, None)
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.s.j.s.h.ContextHandler - Started o.s.j.s.ServletContextHandler@1fe1d70d{/metrics/json,null,AVAILABLE,@Spark}
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.a.s.s.i.SharedState - loading hive config file: file:/Users/elainetuang/Git/dtvengeance/target/scala-2.11/classes/hive-site.xml
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.a.s.s.i.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('/user/hive/warehouse/legal/').
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.a.s.s.i.SharedState - Warehouse path is '/user/hive/warehouse/legal/'.
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.s.j.s.h.ContextHandler - Started o.s.j.s.ServletContextHandler@383d2350{/SQL,null,AVAILABLE,@Spark}
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.s.j.s.h.ContextHandler - Started o.s.j.s.ServletContextHandler@19d7f27{/SQL/json,null,AVAILABLE,@Spark}
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.s.j.s.h.ContextHandler - Started o.s.j.s.ServletContextHandler@78e2d20f{/SQL/execution,null,AVAILABLE,@Spark}
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.s.j.s.h.ContextHandler - Started o.s.j.s.ServletContextHandler@392a65b0{/SQL/execution/json,null,AVAILABLE,@Spark}
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.s.j.s.h.ContextHandler - Started o.s.j.s.ServletContextHandler@45d73052{/static/sql,null,AVAILABLE,@Spark}
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.a.s.s.h.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] h.metastore - Trying to connect to metastore with URI thrift://dev2:9083
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] h.metastore - Connected to metastore.
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.a.h.h.q.s.SessionState - Created local directory: /var/folders/dn/2sn7_b997mqg428h3xf_983w0000gn/T/elainetuang
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.a.h.h.q.s.SessionState - Created local directory: /var/folders/dn/2sn7_b997mqg428h3xf_983w0000gn/T/87b0ef91-2c85-423b-889b-0d00c2f1a8d3_resources
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/elainetuang/87b0ef91-2c85-423b-889b-0d00c2f1a8d3
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.a.h.h.q.s.SessionState - Created local directory: /var/folders/dn/2sn7_b997mqg428h3xf_983w0000gn/T/elainetuang/87b0ef91-2c85-423b-889b-0d00c2f1a8d3
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/elainetuang/87b0ef91-2c85-423b-889b-0d00c2f1a8d3/_tmp_space.db
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.a.s.s.h.c.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /user/hive/warehouse/legal/
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.a.h.h.q.s.SessionState - Created local directory: /var/folders/dn/2sn7_b997mqg428h3xf_983w0000gn/T/d0f51bb9-b335-49a7-8bbb-cf4f0916bc63_resources
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/elainetuang/d0f51bb9-b335-49a7-8bbb-cf4f0916bc63
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.a.h.h.q.s.SessionState - Created local directory: /var/folders/dn/2sn7_b997mqg428h3xf_983w0000gn/T/elainetuang/d0f51bb9-b335-49a7-8bbb-cf4f0916bc63
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/elainetuang/d0f51bb9-b335-49a7-8bbb-cf4f0916bc63/_tmp_space.db
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.a.s.s.h.c.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /user/hive/warehouse/legal/
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.a.s.s.e.s.s.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.a.s.s.e.SparkSqlParser - Parsing command: ods_t_punish_info_7
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.a.s.s.c.p.CatalystSqlParser - Parsing command: int
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.a.s.s.c.p.CatalystSqlParser - Parsing command: int
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.a.s.s.c.p.CatalystSqlParser - Parsing command: string
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.a.s.s.c.p.CatalystSqlParser - Parsing command: string
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.a.s.s.c.p.CatalystSqlParser - Parsing command: string
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.a.s.s.c.p.CatalystSqlParser - Parsing command: date
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.a.s.s.c.p.CatalystSqlParser - Parsing command: string
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.a.s.s.c.p.CatalystSqlParser - Parsing command: string
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.a.s.s.c.p.CatalystSqlParser - Parsing command: string
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.a.s.s.c.p.CatalystSqlParser - Parsing command: string
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.a.s.s.c.p.CatalystSqlParser - Parsing command: string
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.a.s.s.e.d.p.ParquetFileFormat - Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.a.h.m.l.o.FileOutputCommitter - File Output Committer Algorithm version is 1
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.a.s.s.e.d.SQLHadoopMapReduceCommitProtocol - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.a.h.m.l.o.FileOutputCommitter - File Output Committer Algorithm version is 1
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.a.s.s.e.d.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.a.s.s.c.e.c.CodeGenerator - Code generated in 266.477463 ms
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.a.s.SparkContext - Starting job: saveAsTable at WriteHadoopHelplers.scala:11
2019-01-17  INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler - Got job 0 (saveAsTable at WriteHadoopHelplers.scala:11) with 1 output partitions
2019-01-17  INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler - Final stage: ResultStage 0 (saveAsTable at WriteHadoopHelplers.scala:11)
2019-01-17  INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler - Parents of final stage: List()
2019-01-17  INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler - Missing parents: List()
2019-01-17  INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[1] at saveAsTable at WriteHadoopHelplers.scala:11), which has no missing parents
2019-01-17  INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 85.1 KB, free 912.2 MB)
2019-01-17  INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.0 KB, free 912.2 MB)
2019-01-17  INFO [dispatcher-event-loop-3] o.a.s.s.BlockManagerInfo - Added broadcast_0_piece0 in memory on 10.3.0.99:59381 (size: 32.0 KB, free: 912.3 MB)
2019-01-17  INFO [dag-scheduler-event-loop] o.a.s.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1006
2019-01-17  INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at saveAsTable at WriteHadoopHelplers.scala:11) (first 15 tasks are for partitions Vector(0))
2019-01-17  INFO [dag-scheduler-event-loop] o.a.s.s.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
2019-01-17  INFO [dispatcher-event-loop-0] o.a.s.s.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4649 bytes)
2019-01-17  INFO [Executor task launch worker for task 0] o.a.s.e.Executor - Running task 0.0 in stage 0.0 (TID 0)
2019-01-17  INFO [Executor task launch worker for task 0] o.a.h.m.l.o.FileOutputCommitter - File Output Committer Algorithm version is 1
2019-01-17  INFO [Executor task launch worker for task 0] o.a.s.s.e.d.SQLHadoopMapReduceCommitProtocol - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2019-01-17  INFO [Executor task launch worker for task 0] o.a.h.m.l.o.FileOutputCommitter - File Output Committer Algorithm version is 1
2019-01-17  INFO [Executor task launch worker for task 0] o.a.s.s.e.d.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2019-01-17  INFO [Executor task launch worker for task 0] o.a.p.h.c.CodecConfig - Compression: SNAPPY
2019-01-17  INFO [Executor task launch worker for task 0] o.a.p.h.c.CodecConfig - Compression: SNAPPY
2019-01-17  INFO [Executor task launch worker for task 0] o.a.p.h.ParquetOutputFormat - Parquet block size to 134217728
2019-01-17  INFO [Executor task launch worker for task 0] o.a.p.h.ParquetOutputFormat - Parquet page size to 1048576
2019-01-17  INFO [Executor task launch worker for task 0] o.a.p.h.ParquetOutputFormat - Parquet dictionary page size to 1048576
2019-01-17  INFO [Executor task launch worker for task 0] o.a.p.h.ParquetOutputFormat - Dictionary is on
2019-01-17  INFO [Executor task launch worker for task 0] o.a.p.h.ParquetOutputFormat - Validation is off
2019-01-17  INFO [Executor task launch worker for task 0] o.a.p.h.ParquetOutputFormat - Writer version is: PARQUET_1_0
2019-01-17  INFO [Executor task launch worker for task 0] o.a.p.h.ParquetOutputFormat - Maximum row group padding size is 0 bytes
2019-01-17  INFO [Executor task launch worker for task 0] o.a.p.h.ParquetOutputFormat - Page size checking is: estimated
2019-01-17  INFO [Executor task launch worker for task 0] o.a.p.h.ParquetOutputFormat - Min row count for page size check is: 100
2019-01-17  INFO [Executor task launch worker for task 0] o.a.p.h.ParquetOutputFormat - Max row count for page size check is: 10000
2019-01-17  INFO [Executor task launch worker for task 0] o.a.s.s.e.d.p.ParquetWriteSupport - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "ORG",
    "type" : "integer",
    "nullable" : false,
    "metadata" : {
      "name" : "ORG",
      "scale" : 0
    }
  }, {
    "name" : "ID",
    "type" : "integer",
    "nullable" : false,
    "metadata" : {
      "name" : "ID",
      "scale" : 0
    }
  }, {
    "name" : "CORP_NAME",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "name" : "CORP_NAME",
      "scale" : 0
    }
  }, {
    "name" : "REG_NO",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "name" : "REG_NO",
      "scale" : 0
    }
  }, {
    "name" : "CR_WRIT_SN",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "name" : "CR_WRIT_SN",
      "scale" : 0
    }
  }, {
    "name" : "PUNISH_DATE",
    "type" : "date",
    "nullable" : true,
    "metadata" : {
      "name" : "PUNISH_DATE",
      "scale" : 0
    }
  }, {
    "name" : "PUNISH_UNIT",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "name" : "PUNISH_UNIT",
      "scale" : 0
    }
  }, {
    "name" : "PUNISH_ACCORDING",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "name" : "PUNISH_ACCORDING",
      "scale" : 0
    }
  }, {
    "name" : "DEAL_NOTION",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "name" : "DEAL_NOTION",
      "scale" : 0
    }
  }, {
    "name" : "S_EXT_TIMESTAMP",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "name" : "S_EXT_TIMESTAMP",
      "scale" : 0
    }
  }, {
    "name" : "S_EXT_ORG",
    "type" : "string",
    "nullable" : false,
    "metadata" : {
      "name" : "S_EXT_ORG",
      "scale" : 0
    }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  required int32 ORG;
  required int32 ID;
  optional binary CORP_NAME (UTF8);
  optional binary REG_NO (UTF8);
  optional binary CR_WRIT_SN (UTF8);
  optional int32 PUNISH_DATE (DATE);
  optional binary PUNISH_UNIT (UTF8);
  optional binary PUNISH_ACCORDING (UTF8);
  optional binary DEAL_NOTION (UTF8);
  optional binary S_EXT_TIMESTAMP (UTF8);
  required binary S_EXT_ORG (UTF8);
}

       
2019-01-17  INFO [Executor task launch worker for task 0] o.a.h.i.c.CodecPool - Got brand-new compressor [.snappy]
2019-01-17  INFO [Executor task launch worker for task 0] o.a.s.s.e.d.j.JDBCRDD - closed connection
2019-01-17  INFO [Executor task launch worker for task 0] o.a.p.h.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 3223871
2019-01-17  INFO [Executor task launch worker for task 0] o.a.h.m.l.o.FileOutputCommitter - Saved output of task 'attempt_20190117085549_0000_m_000000_0' to hdfs://namenode:8020/user/hive/warehouse/ods_t_punish_info_7/_temporary/0/task_20190117085549_0000_m_000000
2019-01-17  INFO [Executor task launch worker for task 0] o.a.s.m.SparkHadoopMapRedUtil - attempt_20190117085549_0000_m_000000_0: Committed
2019-01-17  INFO [Executor task launch worker for task 0] o.a.s.e.Executor - Finished task 0.0 in stage 0.0 (TID 0). 1565 bytes result sent to driver
2019-01-17  INFO [task-result-getter-0] o.a.s.s.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 47912 ms on localhost (executor driver) (1/1)
2019-01-17  INFO [task-result-getter-0] o.a.s.s.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-01-17  INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler - ResultStage 0 (saveAsTable at WriteHadoopHelplers.scala:11) finished in 47.936 s
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.a.s.s.DAGScheduler - Job 0 finished: saveAsTable at WriteHadoopHelplers.scala:11, took 48.221948 s
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.a.s.s.e.d.FileFormatWriter - Job null committed.
2019-01-17  INFO [pool-1-thread-1-ScalaTest-running-SparkTaskSpec] o.a.s.s.h.HiveExternalCatalog - Persisting file based data source table `default`.`ods_t_punish_info_7` into Hive metastore in Hive compatible format.
2019-01-17  INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo - Removed broadcast_0_piece0 on 10.3.0.99:59381 in memory (size: 32.0 KB, free: 912.3 MB)
2019-01-17  INFO [Thread-2] o.a.s.SparkContext - Invoking stop() from shutdown hook
2019-01-17  INFO [Thread-2] o.s.j.s.AbstractConnector - Stopped Spark@1bd4d050{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-01-17  INFO [Thread-2] o.a.s.u.SparkUI - Stopped Spark web UI at http://10.3.0.99:4040
2019-01-17  INFO [dispatcher-event-loop-0] o.a.s.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
2019-01-17  INFO [Thread-2] o.a.s.s.m.MemoryStore - MemoryStore cleared
2019-01-17  INFO [Thread-2] o.a.s.s.BlockManager - BlockManager stopped
2019-01-17  INFO [Thread-2] o.a.s.s.BlockManagerMaster - BlockManagerMaster stopped
2019-01-17  INFO [dispatcher-event-loop-0] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
2019-01-17  INFO [Thread-2] o.a.s.SparkContext - Successfully stopped SparkContext
2019-01-17  INFO [Thread-2] o.a.s.u.ShutdownHookManager - Shutdown hook called
2019-01-17  INFO [Thread-2] o.a.s.u.ShutdownHookManager - Deleting directory /private/var/folders/dn/2sn7_b997mqg428h3xf_983w0000gn/T/spark-1c971d39-6831-4349-b068-cfcdb8958272
